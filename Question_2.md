**Question 2:**
Briefly explain and implement from scratch the following functions: i) cross-entropy; ii) entropy; iii) mutual information; iv) conditional entropy; v) KL divergence. Take appropriate example toy data/distributions and explain the insights from calculating these quantities.

**Resources:**
https://towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-explained-b09cdae917a
https://tungmphung.com/information-theory-concepts-entropy-mutual-information-kl-divergence-and-more/
https://machinelearningmastery.com/cross-entropy-for-machine-learning/

**Link:**
https://colab.research.google.com/drive/1ocWGpnbGnhPWZejMIs6RJnl3eLr-8hmH?usp=sharing
