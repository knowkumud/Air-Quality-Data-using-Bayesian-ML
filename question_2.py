# -*- coding: utf-8 -*-
"""Question 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ocWGpnbGnhPWZejMIs6RJnl3eLr-8hmH

**Question 2**
i) Cross-entropy:

Cross-entropy is a measure of how different two probability distributions are. It is often used as a loss function in machine learning to measure the difference between predicted and actual probability distributions. Mathematically, it is defined as:

Here's the implementation of cross-entropy from scratch in Python:
"""

import numpy as np

def cross_entropy(p, q):
    """
    Calculate the cross-entropy between two probability distributions p and q.
    """
    return -np.sum(p * np.log(q))

p = np.array([0.6, 0.4])
q = np.array([0.8, 0.2])  

ce = cross_entropy(p, q)
print('Cross-entropy:', ce)

"""ii) Entropy 

Entropy is a measure of the uncertainty or randomness in a probability distribution.
Entropy is a measure of the uncertainty or randomness in a system. In the context of air quality, we can calculate the entropy of a pollutant concentration distribution to understand the spread or uncertainty in its values. For example, if we have data on the concentration of PM2.5 in the air over a week, we can calculate its entropy to understand the level of variation 

where $p$ is a probability distribution over a set of events, and $x$ is an event.

Here's the implementation of entropy from scratch in Python:
"""

import numpy as np 
def entropy(p):
    """
    Calculate the entropy of a probability distribution p.
    """
    return -np.sum(p * np.log(p))

p = np.array([0.7, 0.3])
h = entropy(p)
print('Entropy:', h)

#Defining entropy in terms of air quality 

def entropy(dist):
    p = dist / np.sum(dist)
    return -np.sum(p * np.log2(p))
    
pm25 = np.array([12, 11, 10, 13, 9, 12, 15])
entropy_pm25 = entropy(pm25)
print(f"Entropy of PM2.5 distribution: {entropy_pm25}")

"""**(iii) Mutual Information**

Mutual information is a measure of the dependence between two variables. In the context of air quality, we can calculate the mutual information between the concentration of a pollutant and the meteorological conditions to understand how much information about the pollutant concentration is conveyed by the meteorological conditions. For example, if we have data on the concentration of PM2.5 and the temperature and humidity over a week, we can calculate their mutual information to understand the degree to which the pollutant concentration is affected by the temperature and humidity.
"""

from sklearn.metrics import mutual_info_score

pm25 = np.array([12, 11, 10, 13, 9, 12, 15])
temperature = np.array([20, 21, 22, 23, 24, 25, 26])
humidity = np.array([30, 40, 50, 60, 70, 80, 90])

mi_pm25_temp = mutual_info_score(pm25, temperature)
mi_pm25_humidity = mutual_info_score(pm25, humidity)

print(f"Mutual information between PM2.5 and temperature: {mi_pm25_temp}")
print(f"Mutual information between PM2.5 and humidity: {mi_pm25_humidity}")

"""**iv) Conditional Entropy**

Conditional entropy is the amount of uncertainty in a random variable Y given the value of another random variable X. It is denoted as H(Y|X) and is calculated as the average amount of information needed to describe Y given X. In the context of air quality, we can use conditional entropy to measure how much information about air quality in one location or time tells us about air quality in another location or time.

For example, suppose we have two locations A and B, and we want to compare the air quality measurements in these locations. We can calculate the conditional entropy of the air quality measurements at location B given the air quality measurements at location A. This will tell us how much information about air quality in location A tells us about air quality in location B. If the conditional entropy is low, it means that air quality in location A is a good predictor of air quality in location B.
"""

import numpy as np
from scipy.stats import entropy

# Define the joint distribution of air quality measurements at A and B
joint_dist = np.array([[0.1, 0.2, 0.3],
                       [0.2, 0.3, 0.1],
                       [0.1, 0.1, 0.1]])

# Calculate the marginal distribution of air quality measurements at A
marginal_a = np.sum(joint_dist, axis=1)
marginal_a_sum = np.sum(marginal_a)
if marginal_a_sum == 0:
    conditional_entropy = 0
else:
    marginal_a = marginal_a / marginal_a_sum

# Calculate the conditional distribution of air quality measurements at B given A
conditional_b_given_a = np.zeros_like(joint_dist)
for i in range(joint_dist.shape[0]):
    marginal_b_given_a = np.sum(joint_dist[i])
    if marginal_b_given_a == 0:
        continue
    conditional_b_given_a[i] = joint_dist[i] / marginal_b_given_a

# Calculate the conditional entropy of air quality measurements at B given A
conditional_entropy = entropy(np.sum(conditional_b_given_a, axis=0))
print(f"Conditional entropy: {conditional_entropy}")

import numpy as np
from scipy.stats import entropy

# Suppose we have air quality measurements at two locations A and B
a_values = [10, 20, 30, 40, 50]
b_values = [20, 25, 30, 35, 40]

# Calculate the joint distribution of air quality measurements at A and B
joint_dist = np.zeros((len(a_values), len(b_values)))
for i, a in enumerate(a_values):
    for j, b in enumerate(b_values):
        joint_dist[i, j] = np.sum((a_values == a) & (b_values == b))

# Calculate the marginal distribution of air quality measurements at A
marginal_a = np.sum(joint_dist, axis=1)
if not np.any(marginal_a):
    print("No values satisfy the condition in joint_dist.")
else:
    marginal_a = marginal_a / np.sum(marginal_a)

# Calculate the conditional distribution of air quality measurements at B given A
conditional_b_given_a = joint_dist / marginal_a[:, np.newaxis]

# Calculate the conditional entropy of air quality measurements at B given A
conditional_entropy = entropy(np.sum(conditional_b_given_a, axis=0))
print(f"Conditional entropy: {conditional_entropy}")

import numpy as np
from scipy.stats import entropy

# Suppose we have air quality measurements at two locations A and B
a_values = [10, 20, 30, 40, 50]
b_values = [20, 25, 30, 35, 40]

# Calculate the joint distribution of air quality measurements at A and B
joint_dist = np.zeros((len(a_values), len(b_values)))
for i, a in enumerate(a_values):
    for j, b in enumerate(b_values):
        joint_dist[i, j] = np.sum((a_values == a) & (b_values == b))

# Calculate the marginal distribution of air quality measurements at A
marginal_a = np.sum(joint_dist, axis=1)
if np.any(marginal_a == 0):
    conditional_entropy = 0
else:
    marginal_a = marginal_a / np.sum(marginal_a)

    # Calculate the conditional distribution of air quality measurements at B given A
    conditional_b_given_a = joint_dist / marginal_a[:, np.newaxis]

    # Calculate the conditional entropy of air quality measurements at B given A
    conditional_entropy = entropy(np.sum(conditional_b_given_a, axis=0))

print(f"Conditional entropy: {conditional_entropy}")

"""**(iv) KL Divergence:**

KL divergence is a measure of the difference between two probability distributions. It is used to compare how different two distributions are, and is calculated as the average amount of additional information needed to describe one distribution compared to the other. In the context of air quality, we can use KL divergence to compare the distribution of air quality measurements at different locations or times.

Eg: We have two time periods A and B, and we want to compare the air quality measurements in these time periods. We can calculate the KL divergence of the air quality measurements at time period A with respect to the air quality measurements at time period B. This will tell us how different the air quality measurements in time period A are from those in time period B. If the KL divergence is high, it means that the air quality measurements in time period A are very different from those in time period B.
"""

import numpy as np

# Suppose we have air quality measurements at two time periods A and B
a_values = [10, 20, 30, 40, 50]
b_values = [20, 25, 30, 35, 40]

# Calculate the distribution of air quality measurements in time period A
a_dist = np.array(a_values) / np.sum(a_values)

# Calculate the distribution of air quality measurements in time period B
b_dist = np.array(b_values) / np.sum(b_values)

# Calculate the KL divergence of A with respect to B
kl_div = np.sum(a_dist * np.log2(a_dist / b_dist))

print("KL divergence of A with respect to B:", kl_div)