# -*- coding: utf-8 -*-
"""Question 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DnteSNhNQTytM7_vjlQGbvObWrS74Pxx
"""



"""#Question 4
To prepare the image dataset for binary classification with two classes (Fox v/s Dog), we need to collect a sufficient number of images for each class and split them into training, validation, and test sets. Here are the steps to do this:

Collect images for each class: We can use image search engines such as Google Images, Bing Images, or Flickr to collect images for each class. It's important to choose high-quality images and ensure that there is enough diversity in the images for each class.

Split the dataset into training, validation, and test sets: We should split the dataset into three subsets: training set, validation set, and test set. The training set will be used to train the model, the validation set will be used to tune the hyperparameters, and the test set will be used to evaluate the final performance of the model. A common split is 70% training, 15% validation, and 15% test.

Preprocess the images: We should preprocess the images by resizing them to a common size, normalizing the pixel values, and augmenting the data by applying transformations such as rotation, flipping, and zooming.

Now that we have prepared the dataset, we can use a small CNN to classify the images. Here's an example implementation using the TensorFlow library:
"""



import jax.numpy as jnp
from jax import grad, jit, vmap
import numpy as np
import matplotlib.pyplot as plt
from tensorflow_probability.substrates import jax as tfp

tfd = tfp.distributions

# Define the two-mode distribution as a Mixture of Gaussians
mix = tfd.Mixture(
    cat=tfd.Categorical(probs=[0.3, 0.7]),
    components=[
        tfd.Normal(loc=-2., scale=0.5),
        tfd.Normal(loc=2., scale=1.),
    ]
)

# Define the Normal distribution to be fit
normal = tfd.Normal(loc=0., scale=1.)

# Define the KL divergence between the two distributions
def kl_div(mu):
    # Create a Normal distribution with the given mean
    norm = tfd.Normal(loc=mu, scale=1.)
    # Calculate the KL divergence between the true distribution and the fitted Normal distribution
    kl = tfd.kl_divergence(mix, norm)
    # Return the KL divergence as a numpy array
    return jax.device_get(kl)



# Define the update function to update the mean of the Normal distribution
update_mean = jit(lambda x, lr: x - lr * grad(kl_div)(x))

# Set the learning rate and maximum number of iterations
lr = 0.1
max_iter = 50

# Initialize the parameters and lists to store the iteration-wise progress
mu = 0.
kl_divs = [kl_div(mu)]
mus = [mu]

# Fit the Normal distribution to the two-mode distribution using gradient descent
for i in range(max_iter):
    mu = update_mean(mu, lr)
    kl_divs.append(kl_div(mu))
    mus.append(mu)

# Create the animation of the iteration-wise progress
fig, ax = plt.subplots()
ax.plot(np.linspace(-6, 6, 1000), mix.prob(np.linspace(-6, 6, 1000)), label='Mixture of Gaussians')
for i in range(len(mus)):
    ax.plot(np.linspace(-6, 6, 1000), normal.prob(np.linspace(-6, 6, 1000), loc=mus[i]), alpha=0.5, label='Iteration {}'.format(i))
ax.legend(loc='best')
ax.set_xlabel('x')
ax.set_ylabel('p(x)')
ax.set_title('Fitting a Normal distribution to a two-mode distribution using KL-divergence')
plt.show()



import tensorflow as tf

# Define the model architecture
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(224,224,3)),
    tf.keras.layers.MaxPooling2D((2,2)),
    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2,2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Generate random samples with different shape parameters
n_samples = 250000
alpha_values = np.array([0.5, 1, 2, 5, 10])
s_values = np.array([1, 2, 3, 4, 5])
samples = []
for alpha in alpha_values:
    u = np.random.uniform(size=n_samples)
    x = frechet_icdf(u, alpha, s_values)
    samples.append(x)
# Plot the KDE plot
x_grid = np.linspace(0, 10, 1000)
pdf_values = np.array([frechet_pdf(xi, alpha, s[0]) for xi in x_grid])
kde = gaussian_kde(x.flatten())
kde_values = kde(x_grid)
plt.plot(x_grid, pdf_values, label='PDF')
plt.plot(x_grid, kde_values, label='KDE')
plt.legend()
plt.show()

!pip install modAL

import matplotlib.pyplot as plt

# Plot the active learning iterations v/s test accuracy
plt.plot(range(n_iterations), test_accs)
plt.xlabel('Active Learning Iterations')
plt.ylabel('Test Accuracy')
plt.show()

import matplotlib.pyplot as plt

# Plot the active learning iterations v/s test accuracy
plt.plot(range(n_iterations), test_accs)
plt.xlabel('Active Learning Iterations')
plt.ylabel('Test Accuracy')
plt.show()

import matplotlib.pyplot as plt

# Plot the active learning iterations v/s test accuracy
plt.plot(range(n_iterations), test_accs)
plt.xlabel('Active Learning Iterations')
plt.ylabel('Test Accuracy')
plt.show()

import matplotlib.pyplot as plt

# Plot the active learning iterations v/s test accuracy
plt.plot(range(n_iterations), test_accs)
plt.xlabel('Active Learning Iterations')
plt.ylabel('Test Accuracy')
plt.show()

import matplotlib.pyplot as plt

# Plot the active learning iterations v/s test accuracy
plt.plot(range(n_iterations), test_accs)
plt.xlabel('Active Learning Iterations')
plt.ylabel('Test Accuracy')
plt.show()

from modAL.models import ActiveLearner
from modAL.uncertainty import uncertainty_sampling
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# Load the dataset
train_data = ...
test_data = ...

# Define the CNN architecture
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the active learning strategy
def strategy(learner, X):
    uncertainty = learner.predict(X)
    return uncertainty

# Initialize the active learner
batch_size = 32
learner = ActiveLearner(
    estimator=model,
    X_training=X_train,
    y_training=y_train,
    query_strategy=uncertainty_sampling,
    validation_data=val_data,
    epochs=epochs
)

# Train the model iteratively on the selected images
n_iterations = 10
test_accs = []
for i in range(n_iterations):
    # Select the most informative images to label
    X_pool = test_data[0]
    query_idx = learner.query(X_pool, batch_size=batch_size, n_instances=batch_size, 
                              verbose=0, strategy=strategy)
    X_batch, y_batch = test_data[0][query_idx], test_data[1][query_idx]
    
    # Train the model on the selected images
    model.fit(X_batch, y_batch, epochs=1, verbose=0)
    learner.teach(X=X_batch, y=y_batch, verbose=0)
    
    # Evaluate the model on the test set
    test_loss, test_acc = model.evaluate(test_data[0], test_data[1], verbose=0)
    test_accs.append(test_acc)

# Plot the active learning iterations v/s test accuracy curve
import matplotlib.pyplot as plt
plt.plot(range(n_iterations), test_accs)
plt.xlabel('Active Learning Iterations')
plt.ylabel('Test Accuracy')
plt.show()

"""The resulting plot shows how the test accuracy improves with each iteration of active learning. We can also see how the rate of improvement slows down over time as the model becomes increasingly confident in its predictions and there are fewer uncertain instances left to label.

Overall, active learning can significantly reduce the amount of labeled data required to train a model while achieving comparable or better performance than traditional supervised learning. The choice of active learning algorithm and strategy can have a significant impact on the performance, and it's important to carefully evaluate and compare different approaches for a given dataset and task.

Analyze the results. We can visualize the progress of the optimization by plotting the KL divergence values at each iteration. Here's the code:
"""

# Plot the KL divergence values
fig, ax = plt.subplots()
ax.plot(kl_divergences)
ax.set_title('KL divergence')
ax.set_xlabel('Iteration')
ax.set_ylabel('KL divergence')
plt.show()

# Plot the final fit of the normal distribution
fig, ax = plt.subplots()
ax.plot(x, y, label='True distribution')
ax.plot(x, norm_dist.pdf(x), label='Initial guess')
ax.plot(x, norm.pdf(x, loc=mu, scale=sigma), label='Final fit')
ax.set_title('Fitting a normal distribution to a two-mode distribution')
ax.set_xlabel('x')
ax.set_ylabel('Probability density')
ax.legend()
plt.show()

"""# Plot the final fit of the normal distribution
fig, ax = plt.subplots()
ax.plot(x, y, label='True distribution')
ax.plot(x, norm_dist.pdf(x), label='Initial guess')
ax.plot(x, norm.pdf(x, loc=mu, scale=sigma), label='Final fit')
ax.set_title('Fitting a normal distribution to a two-mode distribution')
ax.set_xlabel('x')
ax.set_ylabel('Probability density')
ax.legend()
plt.show()
"""

# Define the initial plot
ax.plot(x, y, label='True distribution')
ax.plot(x, norm_dist.pdf(x), label='Initial guess')
ax.set_title('Fitting a normal distribution to a two-mode distribution')
ax.set_xlabel('x')
ax.set_ylabel('Probability density')
ax.legend()

# Run the animation
anim = animation.FuncAnimation(fig, animate, frames=n_iters, repeat=False)

# Save the animation as a GIF file
anim.save('normal_fit.gif', writer='imagemagick')